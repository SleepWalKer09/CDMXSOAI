{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Redes Neuronales Artificiales "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Que son?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comunmente conocidas como __Redes Neuronales__, podemos definirlas simplemente como:\n",
    "\n",
    "\n",
    "_\"... un sistema de computación compuesto por una serie de elementos de procesamiento simples y altamente interconectados, que procesan información mediante su respuesta de estado dinámico a entradas externas.\"_ - __Dr. Robert Hecht-Nielsen.__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las Redes Neuoronales Artificiales(ANN), son dispositivos de procesamiento (algoritmos o hardware) que estan modelados de una manera _holgada_ y de mucho menor escala con la estructura de la corteza cerebral de los mamiferos. Una ANN muy grande puede tener cientos o miles de unidades de procesamiento mientras que el cerebro de un mamifero tiene miles de millones de neuronas con un incremento importante dependiendo de la interaccion o la actividad que se este realizando.\n",
    "\n",
    "Al dia de hoy hemos podido replicar el comportamiento de la retina de una manera muy correcta aunque aun estamos lejos de hacerlo al 100%.\n",
    "\n",
    "Entendamos que hay componente matematico muy importante y de alto nivel detras de las ANN, sin embargo un buen desarrollador no necesariamiente tiene que comprenderlo todo para poder entender las arquitecturas de las diversas ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lo basico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las __ANN__ comunmente estan organizadas en capaz/layers. Los **layers** estan compuestos por un numero de **nodos** que contienen un **activation function**. Los patrones son presentados a la **ANN** via el **input layer**, el cual le comunica a los **hidden layers** donde se hara el procesamiento via el sistema de conecciones ponderadas. Los **hidden layers** se conectan al **output layer** en donde el resultado del proceso se muestra.\n",
    "\n",
    "<img src='./ANN.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mayoría de las **ANN** contienen alguna forma de \"regla de aprendizaje/learning rule\" que modifica los pesos de las conexiones según los patrones de entrada con los que se presenta. En cierto sentido, las **ANN** aprenden con el ejemplo al igual que sus contrapartes biológicas; Un niño aprende a reconocer perros de ejemplos de perros.\n",
    "\n",
    "Aunque existen muchos tipos diferentes de reglas de aprendizaje utilizadas por las redes neuronales, esta demostración se ocupa solo de una; La regla delta. La regla delta a menudo es utilizada por la clase más común de **ANN** denominadas 'redes neuronales de contraportación/backpropagation' (BPNN). Backpropagation es una abreviatura para la propagación del error hacia atrás.\n",
    "\n",
    "Con la regla delta, al igual que con otros tipos de propagación hacia atrás/__backpropagation__, el \"aprendizaje\" es un proceso supervisado que ocurre con cada ciclo o \"época/epoch\" (es decir, cada vez que la red se presenta con un nuevo patrón de entrada) a través de un flujo de activación de salidas hacia adelante. y la propagación de errores hacia atrás de los ajustes de peso. Más simplemente, cuando una red neuronal se presenta inicialmente con un patrón, hace una \"suposición\" al azar de lo que podría ser. Luego ve qué tan lejos estaba su respuesta de la real y hace un ajuste apropiado a sus pesos de conexión. Más gráficamente, el proceso se ve algo así:\n",
    "\n",
    "<img src='./Backpropagation-Learning.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También ten en cuenta que dentro de cada nodo de capa oculta hay una función de activación sigmoidal/__sigmoidal activation__ que polariza la actividad de la red y la ayuda a estabilizarse.\n",
    "\n",
    "La propagación hacia atrás/__backpropagation__ realiza un descenso de gradiente dentro del espacio vectorial de la solución hacia un \"mínimo global/__global minimum__\" a lo largo del vector más empinado de la superficie de error. El mínimo global/__global minimum__ es esa solución teórica con el menor error posible. La superficie del error en sí es un hiperparaboloide, pero rara vez es \"suave\" como se muestra en el gráfico a continuación. De hecho, en la mayoría de los problemas, el espacio de la solución es bastante irregular con numerosos \"pozos\" y \"colinas\" que pueden hacer que la red se establezca en un \"mínimo local\" que no es la mejor solución general.\n",
    "\n",
    "<img src='delta_rule.gif'>\n",
    "<img src='bprop.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que la naturaleza del espacio de error no se puede conocer a priori, el análisis de redes neuronales a menudo requiere un gran número de ejecuciones individuales para determinar la mejor solución. La mayoría de las reglas de aprendizaje tienen términos matemáticos incorporados para ayudar en este proceso que controla la \"velocidad\" (Beta-coeficiente) y el \"impulso\" del aprendizaje. La velocidad de aprendizaje es en realidad la tasa de convergencia entre la solución actual y el mínimo global. Momentum ayuda a la red a superar obstáculos (mínimos locales) en la superficie del error y se establece en o cerca del mínimo global.\n",
    "\n",
    "Una vez que una red neuronal se \"entrena\" a un nivel satisfactorio, se puede utilizar como una herramienta analítica en otros datos. Para hacer esto, el usuario ya no especifica ninguna ejecución de capacitación y, en cambio, permite que la red funcione solo en modo de propagación hacia adelante. Las nuevas entradas se presentan al patrón de entrada donde se filtran y son procesadas por las capas intermedias como si el entrenamiento se llevara a cabo, sin embargo, en este punto, la salida se conserva y no se produce ninguna propagación hacia atrás. La salida de una ejecución de propagación hacia adelante es el modelo predicho para los datos que luego se pueden utilizar para un análisis e interpretación adicional.\n",
    "\n",
    "También es posible sobreentrenar una red neuronal, lo que significa que la red ha sido entrenada exactamente para responder a un solo tipo de entrada; que es muy parecido a la memorización de memoria. Si esto sucede, el aprendizaje ya no puede ocurrir y se hace referencia a la red como \"abuela\" en la jerga de la red neuronal. En aplicaciones del mundo real, esta situación no es muy útil, ya que se necesitaría una red de abuelas separada para cada nuevo tipo de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferencias entre las ANN y la computación convencional?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprender mejor la computación neuronal artificial, es importante saber primero cómo una computadora 'serial' convencional y su información de proceso de software. Una computadora en serie tiene un procesador central que puede abordar un conjunto de ubicaciones de memoria donde se almacenan los datos y las instrucciones. Los cálculos se realizan mediante la lectura de una instrucción por parte del procesador, así como los datos que la instrucción requiere de las direcciones de memoria, la instrucción se ejecuta y los resultados se guardan en una ubicación de memoria específica, según sea necesario. En un sistema en serie (y uno paralelo estándar también) los pasos computacionales son deterministas, secuenciales y lógicos, y el estado de una variable dada puede rastrearse de una operación a otra.\n",
    "\n",
    "En comparación, las ANN no son secuenciales ni necesariamente deterministas. No hay procesadores centrales complejos, sino que hay muchos procesadores simples que generalmente no hacen más que tomar la suma ponderada de sus aportes de otros procesadores. Las ANN no ejecutan instrucciones programadas; responden en paralelo (simulado o real) al patrón de entradas que se le presentan. Tampoco hay direcciones de memoria separadas para almacenar datos. En su lugar, la información está contenida en el estado de activación general de la red. El \"conocimiento\" está representado por la propia red, que es, literalmente, más que la suma de sus componentes individuales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En que se usan las ANN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales son aproximadores universales, y funcionan mejor si el sistema que está utilizando para modelar tiene una alta tolerancia al error. ¡Por lo tanto, no se recomienda que utilice una red neuronal para equilibrar el talonario de cheques! Sin embargo funcionan muy bien para:\n",
    "\n",
    "\n",
    "    - Capturar asociaciones o descubrir regularidades dentro de un conjunto de patrones;\n",
    "    - Donde el volumen, número de variables o diversidad de datos es muy grande;\n",
    "    - Las relaciones entre las variables se entienden vagamente; o,\n",
    "    - Las relaciones son difíciles de describir adecuadamente con los enfoques convencionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitantes de las ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay muchas ventajas y limitaciones en el análisis de redes neuronales y para tratar este tema adecuadamente, tendríamos que analizar cada tipo de red individual, lo que no es necesario para esta discusión general. Sin embargo, en referencia a las redes backpropagational, hay algunos problemas específicos que los usuarios potenciales deben conocer.\n",
    "\n",
    "    - Las redes neuronales de contrapropagación (y muchos otros tipos de redes) son, en cierto sentido, las últimas \"cajas negras\". Además de definir la arquitectura general de una red y tal vez inicialmente sembrarla con números aleatorios, el usuario no tiene otra función que alimentar la entrada y verla entrenar y esperar la salida. De hecho, se ha dicho que con la propagación hacia atrás, \"casi no sabes lo que estás haciendo\". Algunos paquetes de software disponibles gratuitamente (NevProp, bp, Mactivation) permiten al usuario probar el \"progreso\" de las redes a intervalos regulares de tiempo, pero el aprendizaje en sí mismo progresa por sí solo. El producto final de esta actividad es una red entrenada que no proporciona ecuaciones ni coeficientes que definan una relación (como en regresión) más allá de sus propias matemáticas internas. La red 'ES' la ecuación final de la relación.\n",
    "\n",
    "    - Las redes de backpropagational también tienden a ser más lentas de entrenar que otros tipos de redes y algunas veces requieren miles de épocas. Si se ejecuta en un sistema informático verdaderamente paralelo, este problema no es realmente un problema, pero si la BPNN se está simulando en una máquina serie estándar (es decir, un solo SPARC, Mac o PC), la capacitación puede llevar algún tiempo. Esto se debe a que la CPU de las máquinas debe calcular la función de cada nodo y la conexión por separado, lo que puede ser problemático en redes muy grandes con una gran cantidad de datos. Sin embargo, la velocidad de la mayoría de las máquinas actuales es tal que esto no suele ser un problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de las ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependiendo de la naturaleza de la aplicación y la solidez de los patrones de datos internos, generalmente se puede esperar que una red se capacite bastante bien. Esto se aplica a problemas donde las relaciones pueden ser bastante dinámicas o no lineales. Las ANN proporcionan una alternativa analítica a las técnicas convencionales que a menudo están limitadas por supuestos estrictos de normalidad, linealidad, independencia variable, etc. Debido a que una ANN puede capturar muchos tipos de relaciones, permite al usuario modelar de manera rápida y relativamente fácil fenómenos que de otra manera podrían haber sido muy complejos. Difícil o imposible de explicar de otra manera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A codear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # 'Fiajamos'/Seed el generador de numeros aleatorios para que\n",
    "        #  siempre genere el mismo numero cada vez que corremos el codigo.\n",
    "        random.seed(1)\n",
    "        \n",
    "        # Modelamos una sola neurona, con 3 conecciones de entrada y 1 de salida.\n",
    "        # Asignamos ponderaciones aleatorias a una matriz de 3 x 1, con valores en un rango de -1 a 1\n",
    "        # y media 0\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # La función sigmoidea, que describe una curva en forma de S.\n",
    "    # Pasamos la suma ponderada de las entradas a través de esta función para\n",
    "    # normalizarlas entre 0 y 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "    \n",
    "    # La derivada de la función sigmoidea. \n",
    "    # Este es el gradiente de la curva sigmoidea. \n",
    "    # Indica cuánta confianza tenemos sobre la ponderacion existente.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    # Entrenamos la red neuronal a través de un proceso de prueba y error.\n",
    "    # Ajustando los pesos sinápticos cada vez.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pasamos nuestro training set a través de nuestra red neuronal (una sola neurona).\n",
    "            output = self.think(training_set_inputs)\n",
    "            # Calcula el error (La diferencia entre el output deseado\n",
    "            # y el output previsto).\n",
    "            error = training_set_outputs - output\n",
    "            # Multiplica el error por el input y nuevamente por el gradiente de la curva Sigmoide.\n",
    "            # Esto significa que las ponderaciones con menos confianza se ajustan más.\n",
    "            # Esto significa que los inputs, que son cero, no causan cambios en los pesos.\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "            # Ajustamos la ponderacion\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # La red ANN comienza a pensar\n",
    "    def think(self, inputs):\n",
    "        # Pasa los inputs por nuestra ANN (una sola neurona)\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[0.99993704]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Iniciamos una sola red neuronal neuronal.\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "    \n",
    "    # El Training set. Tenemos 4 ejemplos, cada uno consta de 3 inputs\n",
    "    # y 1 output.\n",
    "    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "    \n",
    "    # Entrena la red neuronal utilizando el training set.\n",
    "    # Hazlo 10,000 veces y haz pequeños ajustes cada vez.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "    print(\"New synaptic weights after training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # Probar la red neuronal con una nueva situación. \n",
    "    print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print(neural_network.think(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La version corta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99993704]\n"
     ]
    }
   ],
   "source": [
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "random.seed(1)\n",
    "synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "for iteration in range(10000):\n",
    "    output = 1 / (1 + exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "    synaptic_weights += dot(training_set_inputs.T, (training_set_outputs - output) * output * (1 - output))\n",
    "print(1 / (1 + exp(-(dot(array([1, 0, 0]), synaptic_weights)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como interpretamos esto?\n",
    "\n",
    "    - Dado que el dataset es ridiculamente chico toma milisegundos en procesar\n",
    "    - Nuestas ponderaciones se actualizaron solos a traves de 10000 iteraciones (backpropagation)\n",
    "    - Cuando el proceso termino nos dijo que el resultado seguramente era un 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de todo esto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con todo lo anterior podemos concluir que las __ANN__ son utiles en una gran variedad de tareas y en casos muy especificos sobrepasan a sus contrapartes mamiferas en tareas que creiamos que solo nosotros podiamos hacer aun nivel muy elevado, como:\n",
    "\n",
    "    - Vision\n",
    "    - Text to speech\n",
    "    - Speech to text\n",
    "    \n",
    "Todo esto se logra sin la necesidad de una manifestacion fisica de las __ANN__ y solo esta representada en transistores a base de silicio!\n",
    "\n",
    "Para recordar:\n",
    "\n",
    "    - Una ANN es un algoritmo(basado en la biologia mamifera) que aprende a identificar patrones en un dataset\n",
    "    \n",
    "    -  Backpropagation es una tecnica para entrenar una ANN acutalizando las ponderaciones via una gradiente descendiente\n",
    "    \n",
    "    - Deep learning = ANN con cientos o miles de layers + Big Data + Big Compute\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
